"""This file defines the class NgramModel. It accepts a list of tokenized sentences (in the form generated by CorpusReader.sents()).
It removes punctuation tokens, converts all words to lower case, adds 1 sentence-end token (</s>) at the end of each sentence, adds 
n-1 sentence-start tokens (<s>) at the beginning of each sentence. It Uses the result to pre-calculate and store the frequency tables
needed by the methods listed below. Store the frequency tables in a variable inside the class. It also includes the method probability,
which returns the n-gram probability of the item we pass it, given the corpus; the method sentence perplexity, which returns the perplexity
of a given sentence; and the method choose successor, which probabilistically chooses a successor using the model.
authors: Rita Ruano, Adrian Wojcik, Marilea Canul"""

#Importing necessary modules and methods from said modules 
import re
from corpusreader import CorpusReader
import random

class NgramModel:
    """Accepts a list of tokenized sentences in the form generated by CorpusReader.sents(), and a number which indicates how long the n-grams
    will be."""
    def __init__(self,tokenizedsentences,ngramcount):
        self.tokenizedsentences=tokenizedsentences
        self.ngramcount=ngramcount
        #Calling the later methods from within the initializer, to keep code more organized
        self.formatting= self.formatteddata()     
        self.frequency=self.frequencytables()
        self.unigram=self.unigram()
        
    def formatteddata(self):
        """Removes punctuation from the words within the sentences, makes them into lowercase, adds begginign and end of sentence markers."""
        #Copies the original the tokenized sentences list to avoid editing the original permanently
        copyoftokenizedsentences=self.tokenizedsentences.copy()
        #Initializes empty list where the formatted data will be added to
        formattedsentences=[]
        #Loops over each sentence of the copied list
        for sentence in copyoftokenizedsentences:
            #Loops over each word of the sentences
            for  word in sentence:
                #If the looped-over word is composed of one or more non-aplhanumeric character(s), it is removed from the list
                if re.match('\W+',word):
                    sentence.remove(word)    
            #Words are converted to lowercase
            sentence = [word.lower() for word in sentence]
            #An end of sentence marker (</s>) is added at the end of each sentence
            sentence.append('</s>')
            #Beginning of sentence marker(s) (<s>) are added at the beginning of each sentence.
            #The amount of markers depends on the lenght of the ngrams we plan to create
            for i in range (0,self.ngramcount-1):
                sentence.insert(0,'<s>')
            #The formatted sentences are appendend to the final list
            formattedsentences.append(sentence)
        return formattedsentences     
        
        
    def frequencytables(self):
        """Pre-calculates and stores the frequency tables needed for the later defined methods. Takes a number, the ngram count.
        Returns: dictionary with the n-grams divided as per the number given in self.ngramcount, and respective counts of occurences
        in the corpus."""
        #Initializes list where n-grams will be appended to
        ngramlist=[]
        #Loops over each sentence that has been previously formatted
        for sentence in self.formatting:
            #Creates the first n-gram depending on the ngramcount that was specified
            x=0
            y=self.ngramcount
            #While the y value is shorter than the last indexed item of the sentence, it creates all the possible n-grams 
            #by adding 1 to each coordinate of the index slice, and appends them to the n-gram list. Accounts for index being one short of the 
            #total length of the sentence
            while y<len(sentence):
                ngramlist.append(tuple(sentence[x:y]))
                x+=1
                y+=1
        #Initiates the frequency dictionary
        frequencytable={}      
        #Loops over each n-gram on the list
        for ngram in ngramlist:
            #If it is already on the dictionary, it counts one more on the value of said n-gram entry
            if ngram in frequencytable:
                frequencytable[ngram]+=1
            #If it is not yet in the dictionary, it is added with a count of one
            else:
                frequencytable[ngram]=1   
        #Returns the frequency dictionary     
        return frequencytable    
            
            
    def unigram(self):
        """"Creates a unigram dictionary in order to later calculate the number of unique tokens in the corpus.
        In a similar way as frequencytables function."""
        #Initiates the dictionary
        unigrams={}
        #Loops through the formatted sentences
        for sentence in self.formatting:
            #Loops through each token
            for token in sentence:
                #Adds one to the count if it is already in the dictionary
                token=tuple([token])
                if token in unigrams:
                    unigrams[token]+=1
                #Creates an entry otherwise
                else:   
                    unigrams[token]=1
        return unigrams       
            
    
    def probability(self,ngram,smoothing_constant=0.0):
        """Returns the probability of a given n-gram considering the whole corpus. If not specified, assumes a smoothing_constant of 0.0"""
        #Sum of the unique ngrams that share the same prefix
        sumofprefix=0
        #Sum of all of the counts of each individual ngram that share the same prefix
        sumofprefixcounts=0
        #Calls the frequency dictionary that was previously created from the corpus
        currentdictionary=self.frequency
        #Checks if the ngram exists in the dictionary
        if ngram in currentdictionary.keys():
            #If it does, loops over the keys
            for key in currentdictionary.keys():
                #The prefix is composed of all tokens except the last. If the prefix of the key matches the one of the ngram that was given, adds the counts of that ngram to the counter of all occurences of that prefix
                if ngram[:-1]==key[:-1]:
                    sumofprefixcounts+=currentdictionary[key]
                    sumofprefix+=1
            #If the smoothing constant is not specified, it uses the default value and calculates the raw probability
            if smoothing_constant==0.0:
                rawprobability= self.frequency[ngram]/sumofprefixcounts
                return rawprobability
            # If the smoothing constant is specified, it uses that value and calculates the k-smoothed probability, which also uses the individual uses of the prefix
            else:
                ksmoothedprobability= (self.frequency[ngram]+smoothing_constant)/((sumofprefixcounts+smoothing_constant)*len(self.unigram))
                return ksmoothedprobability
        #If the ngram is not found in the corpus, it returns 0.0 or the associated smoothed probability
        else:   
            return smoothing_constant
        

    def perplexity(self,sentence, smoothing_constant=1.0):
        """Given a sentence in the form of a list of tokens, formats it the same way that formatteddata() does,
        removing punctuation-only tokens, changing it to lowercase, and surrounding with <s> and </s>). 
        It calculates and returns the sentence's perplexity using the add-k-smoothed probability model. If unknown
        words are seen, returns infinite."""
        # Creates a copy of the sentence to avoid permanently mutating the data
        copyofsentence=sentence.copy()
        #Loops over each word in the given sentence and removes punctuation
        for word in copyofsentence:
            if re.match('\W+',word):
                copyofsentence.remove(word)    
        #Converts all words to lowercase
        copyofsentence = [word.lower() for word in copyofsentence]
        #Adds a sentence-end token (</s>) at the end of the sentence
        copyofsentence.append('</s>')
        #Beginning of sentence marker(s) (<s>) are added at the beginning of the sentence
        #The amount of markers depends on the lenght of the ngrams we plan to create
        for i in range (0,self.ngramcount-1):
            sentence.insert(0,'<s>')
        #Initiating perplexity variable as 1, given that it will be multiplied instead of summed
        perplexity=1
        #Creates a list for the ngrams of the sentence
        ngramlist=[]
        x=0
        y=self.ngramcount
        #While the y value is shorter than the last indexed item of the sentence, it creates all the possible n-grams 
        #by adding 1 to each coordinate of the index slice, and appends them to the n-gram list
        while y<len(copyofsentence):
            ngramlist.append(tuple(copyofsentence[x:y]))
            x+=1
            y+=1
        #Loops over tokens, and if one is not present in the corpus returns infinite perplexity
        for token in copyofsentence:
            token=tuple([token])
            if token not in self.unigram:
                return float('inf')
        #Otherwise loops through the ngrams of the list
        else:
            for ngram in ngramlist:
                #Calculates the probability of each ngram
                ngramprobability=self.probability(ngram,smoothing_constant)
                #Multiplies the probabilities of the ngrams one by one to initiate the calculation of perplexity
                perplexity=perplexity*ngramprobability
            #When the loop is done, finishes calculating perplexity
            perplexity=perplexity**-(1/(len(ngramlist)))
            return perplexity


    def choose_successor(self, prefix):
        """Given a list of tokens, takes into account the last item as prefixe for a future bigram. Selects a successor for the completion of the bigram,
        by looking at possibilities within the corpus."""
        #Stores the token which will be the prefix for the bigram
        successorprefix=prefix[-1]
        #Stores the dictionary that was created based on the given corpus
        currentdictionary=self.frequency
        #Initiates a list of the possible successors
        possiblesuccessors=[]
        #loops through the keys in the dictionary
        for key in currentdictionary.keys():
            #If the bigram prefix is the same as the second last item in the key
            if successorprefix==key[-2]:
                #Appends the token that follows it to the possibilities list
                possiblesuccessors.append(key[-1])
        #After, it chooses one possibility from the list to complete the bigram
        successorchoice=random.choices(possiblesuccessors)
        if successorchoice is not None:
            return successorchoice
        #If the list is empty returns None
        else:
            return None
        