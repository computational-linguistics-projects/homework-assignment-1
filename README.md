HOMEWORK 1: Ngram Models
DUE at 23:59 on Sunday, May 18, 2025

Your goal in this assignment is to build a stochastic text generator based on an ngram model:

Build an ngram model for a corpus.
Implement add-k smoothing on your model, and use it to calculate the perplexity of some text.
Use your model in reverse, to generate random text based on the transition probabilities in the models.
Read the instructions carefully. Many of the necessary concepts and skills are covered in the forthcoming reading and practica (up to and including Practicum 6).

Projects will be graded on function (correct results, design according to the specification) as well as form (organization and appropriateness of the code, documentation). Test the parts of your program to ensure that they work as intended.



- here we can add later the division of labour and maybe also a sort of checklist of what we need to do by steps to complete the assignment


#TODO 
main
- create your own branch for coding under the main thingy

  
ASSIGNMENT TEXT:

HOMEWORK 1: Ngram Models
DUE at 23:59 on Sunday, May 18, 2025

Your goal in this assignment is to build a stochastic text generator based on an ngram model:

Build an ngram model for a corpus.
Implement add-k smoothing on your model, and use it to calculate the perplexity of some text.
Use your model in reverse, to generate random text based on the transition probabilities in the models.
Read the instructions carefully. Many of the necessary concepts and skills are covered in the forthcoming reading and practica (up to and including Practicum 6).

Projects will be graded on function (correct results, design according to the specification) as well as form (organization and appropriateness of the code, documentation). Test the parts of your program to ensure that they work as intended.

Recommendations on working on the assignment
Read the entire assignment before getting started
Re-read the relevant textbook parts (see below)
You will likely find that blindly splitting up the task and doing the parts independently will not work very well, as many parts of the assignment depend on each other. Pair-programming (sitting together in person or online (screen-sharing) with one person typing) is often effective. There is also probably some splitting up that can be done, but it needs to be done thoughtfully.
For testing, make tiny examples in the right format to work from, even if the code that should generate them doesn’t exist yet. This will allow you to test parts of the code independently.
The assignment has some things very clearly specified (such as what some functions are called, what format the data needs to be in, etc.), but also some things are more general, where you’re told what result you need, but not how to get there. This is part of the fun – you need to figure out how to accomplish the task, not just how to write the code.
DETAILS
We provide you with a small corpus (part of the Gutenberg collection of texts) for you to test your code with. It is in train.zip, and you should unzip it and put it in the same directory as your modules. (Don’t hand it in, though.)

Your code must provide three modules:

1. Module corpusreader.py, which defines the class CorpusReader.
Base it on the CorpusReader class from the Activity-CorpusReader notebook, but make the following changes:

It should provide an initializer that accepts any directory path name for the corpus it will read.
It should have a method sents() that will return the text of the corpus as a list of tokenized sentences, i.e., the corpus becomes a list of lists of tokens. (See Additional Specifications below for full instructions.)
The CorpusReader class doesn’t need to have words() or lines() methods, as they won’t be used. It’s up to you if it uses a helper function like _get_all_text().
You should be able to define an instance of a CorpusReader and its sentences as follows:

corpus = CorpusReader("path/to/corpus/directory")
sentences = corpus.sents()  # a list of lists of tokens
Your corpus reader will be tested with different path names. Make sure it works. Make sure as well that when you use it, you have no absolute paths and use forward slashes only (/).

2. Module model.py, which defines the class NgramModel.
Its initializer (__init__) must:
accept a list of tokenized sentences (in the form generated by CorpusReader.sents()),
remove all tokens that consist entirely of punctuation,
convert all words to lower case,
add 1 sentence-end token (</s>) at the end of each sentence (see Jurafsky & Martin).
add n-1 sentence-start tokens (<s>) at the beginning of each sentence,
Use the result to pre-calculate and store the frequency tables needed by the methods listed below.
Store the frequency tables in a variable inside the class.
Note: Do not concatenate the sentences back into one long string; these ngrams are calculated on sentences, not across sentence boundaries.
A frequency table stores the raw counts of the ngrams (e.g. for trigrams, if "the cat" is followed by "slept" 7 times, there should be a 7 in there.
This encodes how many time a prefix of words of length n-1 was followed by a word
dictionaries are efficient for storing this kind of information, but you can do it any way you like as long as it doesn’t take forever to build or use
Note that lists are unhashable, so if you use a dictionary, you’ll need to make the keys something else (e.g. tuples of the first n-1 words, or you can re-concatenate them with a safe delimiter).
You will probably want additional things stored in your model. It’s up to you to figure out what you need!
While your modules and classes can have additional functions, variables, etc. defined, they must also work exactly as specified in the assignment. For example, no adding new parameters to functions (unless you give them a default value), no choosing different names for functions, etc.)
Try to avoid storing the raw dataset itself in the model. It makes it very a large object, and once you build your model, it’s redundant.
If you create a CorpusReader called corpus, then an ngram model is created like this:

my_model = NgramModel(corpus.sents(), n)
e.g. a bigram model: my_bigram_model = NgramModel(corpus.sents(), 2)
The NgramModel class must also provide the following methods with the following parameters and defaults:

probability(self, ngram, smoothing_constant=0.0)
If smoothing_constant = 0.0, return Praw(wn|w0, ..., wn − 1), the raw probability of seeing token wn if we have just seen those n − 1 tokens. (Make this the default.)
Otherwise, return PAdd − k(wn|w), the add-k-smoothed probability of seeing token wn if we have just seen token w, letting k= smoothing_constant
If any unknown words are seen, return 0.0.
perplexity(self, sentence, smoothing_constant=1.0)
Given a sentence in the form of a list of tokens, clean it up the same way the training input was cleaned up (remove punctuation-only tokens, change to lowercase, surround with <s> and </s>), and return its perplexity using the add-k-smoothed probability model.
If any unknown words are seen, return infinity (you can get this with float("inf")).
Pass the given smoothing_constant to the probability method for add-k-smoothing, default 1.0 (Making the default LaPlace smoothing)
choose_successor(self, prefix)
given a list of words of length n − 1, probabilistically choose a successor using the model
if word is not in the model, return None (not the string "None"! You can use return or return None)
hint: random.choices is very helpful here
All functions should work correctly if called with unknown tokens, returning zero probability, infinite perplexity, or None as appropriate

3. Module generate.py, which is the main module of the system. It should import the other two modules, and consists of two sections: A regular module section with function definitions etc., and an executable section protected by if __name__ == '__main__'.
The definitions section should define the function generate_sentence(ngram_model), which should:
take an NgramModel object,
return (not print) a generated sentence as a list of tokens, and
rely on the method NgramModel.choose_successor for its work.
The executable section should do three things:

Construct a NgramModel from a corpus specified on the command line, or a default (see below).

Use generate_sentence to construct and print out two random sentences. Format them so they’re human-readable. Print the second sentence on a new line, e.g.

    >This is a sentence.
    >
    >This is another sentence.
For each of the following sentences, tokenize them with nltk.word_tokenize() (see below), and use the model’s perplexity method to calculate and print out the sentence and perplexity. Use the default smoothing constant of 1.0.

Suggestive, Watson, is it not?
It is amazing that a family can be torn apart by something as simple as a pack of wild dogs!
So spoke Sherlock Holmes and turned back to the great scrapbook in which he was arranging and indexing some of his recent material.
What I like best about my friends is that they are few.
Friends what is like are they about I best few my that.
Make the printing well formatted and readable, adding section titles, spaces, etc. for ease of use. This script (and only this script), when run as the main program, should actually call the functions that read the corpus, build the model, etc. It should be run from the command line like this:

python generate.py path/to/corpus/directory/
or on some systems

python3 generate.py path/to/corpus/directory/
If run without an argument, as below, it should assume the corpus is at the path "./train" (a directory called train inside the folder containing your Python files), but do not assume anything about the files included in the corpus. Do not use any absolute paths in your code.

python generate.py 
python3 generate.py 
ADDITIONAL SPECIFICATIONS
Follow these specifications; deviations are errors, even if they are intentional (e.g., because you believe that a different design is better.) But your modules may (and probably should) define methods, functions and classes in addition to the ones described above.

When importing a module we expect definitions, not computation. The modules corpusreader and model must not produce output, read files, or raise errors when imported. (Optional: If you wish, you may provide conditional code that runs only when you execute a module as the main program, i.e. protected by if __name__ == "__main__".)

Name your variables, functions, etc. with interpretable names. It is traditional to use letters like i,j,k, and n for integers, and that is fine if you want, but otherwise your variable names should be meaningful (not x!)

Don’t leave stray commented-out code in your files. If you have a reason to include it, that’s fine, just write a comment that explains it.

Make sure your class methods take self as their first argument. (Advanced: or, if you’re intentionally using static methods, please add a @staticmethod decorator.)

As you saw in your first Jupyter Notebook using NLTK, the first time you use NLTK you may need to download some files. Please don’t include that code in your assignment. You can assume your reader is all set up.

Corpus reader format:
We have seen that we can read a corpus as a list of lines, or as a list of sentences. But while we have so far represented each sentence as a string, we will now use a more expressive data structure: A sentence is a list of tokens, meaning that the aggregated sentences are stored as a list of lists of tokens. This format is more useful for NLP tasks, and in particular for our ngram task.

Use nltk.sent_tokenize as in the Activity-searching notebook to turn the raw text into a list of sentence strings. To split the sentence into words, you’re going to use a new-to-you built-in function from NLTK called nltk.word_tokenize. It takes a sentence string as argument and returns a list of strings, where each string is a token. Use these two functions together to get a list of lists of tokens.

For example, a tiny toy corpus as returned by sents()might look like:


[['``', 'Well', ',', 'well', ',', 'Mrs.', 'Warren', ',', 'let', 'us', 'hear', 'about', 'it', ',', 'then', '.'], ['You', 'do', "n't", 'object', 'to', 'tobacco', ',', 'I', 'take', 'it', '?'], ['Thank', 'you', ',', 'Watson', '--', 'the', 'matches', '!'], ['You', 'are', 'uneasy', ',', 'as', 'I', 'understand', ',', 'because', 'your', 'new', 'lodger', 'remains', 'in', 'his', 'rooms', 'and', 'you', 'can', 'not', 'see', 'him', '.'], ['Why', ',', 'bless', 'you', ',', 'Mrs.', 'Warren', ',', 'if', 'I', 'were', 'your', 'lodger', 'you', 'often', 'would', 'not', 'see', 'me', 'for', 'weeks', 'on', 'end', '.', "''"]]
Like in the CorpusReader notebook, your corpus reader should handle errors when given a non-existent corpus path.

Perplexity definition
The definition of perplexity can be interpreted in a couple of different ways, yielding slightly different results. For your assignment, consider the sentence to include its end markers <s> and </s>, and normalise by the number of ngrams, following your textbook (Jurafsky and Martin, chapter 3).

Documentation and resources:
Each module, class and non-trivial function must begin with a docstring describing what it does and (for functions and methods) what its arguments must be like. Include the names of the authors in the module docstrings. Every non-trivial mathematical calculation should be accompanied by a comment describing the formula or algorithm, and giving a reference if appropriate. For example, a module called my_module.py could look like this:

"""
my_module prints everyone's names
authors: Meaghan Fowlie and Santa Claus
"""


class Names:
    """
    stores and prints out names
    Attributes:
        self.names : list of strings
    """
    
    def __init__(self, names):
        """
        @param names: list of strings
        stores the names of the group members
        """
        self.names = names
        
    def name_printer(self):
        """
        prints the names stored in names, one on each line
        """
        for name in self.names:
            print(name)
You may utilize python libraries distributed with python, including re, random, and collections. (The function random.choices() and the classes collections.Counter and collections.defaultdict are particularly likely to be useful.) Basically, anything documented in docs.python.org is fair game. You may not use anything from NLTK except sent_tokenize and word_tokenize. If you copy more than small snippets of code, acknowledge the source of the information. Follow the usual rules on academic attribution and plagiarism.

Your code should be the work of your group. You may look on the web for supplementary materials, including forums and tutorials, but you MAY NOT ask a question on any forum. You also may NOT use a language model such as ChatGPT or Google Copilot or the AI tool that comes with Anaconda (see announcement from Week 1 for how to turn it off). You are welcome to come to one of us with any questions.

Ensure that everyone in the group understands at least the basic idea of all of the code. This also means that it should be well-commented enough that we and you can read it, think for a bit, and have an idea of how it works.

Report: report.txt or report.pdf
The final document is a report, saved as report.pdf or report.txt (no Word documents, please!). It should be between about one and three pages long.

This should contain:

Group number and your names
A brief report on the process:
How did you divide up the work, work together, etc?
Did you end up using Git at all? How did that go?
Any problems you encountered (e.g. a sick member, particularly disruptive difficulties with the assignment itself, a member who was unable to contribute after all…)
Answers to the following two questions:
Recall sentences 4 and 5 from the script section of generate.py:

What I like best about my friends is that they are few.
Friends what is like are they about I best few my that.
Why do you think sentence 4 has a much lower perplexity than sentence 5 even though they contain the same words?
Why do we start each sentence with n-1 <s> tokens? (That is, why start tokens, and why n-1 of them?)

Give the report a little structure, e.g. sections, copying in the question to go with the answer, etc, so that it’s readable independently of this assignment file.

Suggestions and hints
Jurafsky and Martin (your main text book) provides an extensive discussion of ngrams and smoothing in Chapter 3. (Note that we’re not using a dev set to choose a good k for add-k smoothing, despite what they recommend!)

The following is all optional.

For debugging, or for good code organisation, you might find it useful to define methods in your NgramModel such as:

print_model: print out all the words, their successors, and their counts (this might be too big to read, but you could also print it to a file)
print_successors: for a word, print out its possible successors and their counts (or report that it’s not in the model)
prepare_tokens: clean up a sentence to make it useful for the model (e.g. adding <s> and </s>)
The perplexities of the sentences 1-5 include at least one sentence that your perplexity function should assign 0.0 to, and at least one that it should assign a positive number to.

Checklist
Please hand in a zip file containing the five (or six, if including a README) assignment files. No corpora, please!

Make sure you include the following files, named exactly as requested.

corpusreader.py providing class CorpusReader
model.py providing class NgramModel
generate.py providing a function generate_sentence and an executable script protected by if __name__ == "__main__"
report.pdf or report.txt
CorpusReader contains (at least) method sents()

NgramModel contains (at least):

a stored frequency table
method probability
method choose_successor
method perplexity
Please don’t include any corpora in your submission.

Note: If your modules throw errors when imported, you will be penalized for that, in addition to the mistakes that led to the errors. If you can’t solve those problems before handing it in, you may comment bad parts out, and provide a file called README.txt or README.md in which you direct our attention to those parts so we can see how close you got to making it work. Use line numbers.

- create your own branch for coding under the main thing
mar's-code
