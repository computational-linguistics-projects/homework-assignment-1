"""This file defines the class NgramModel. It accepts a list of tokenized sentences (in the form generated by CorpusReader.sents()).
It removes punctuation tokens, converts all words to lower case, adds 1 sentence-end token (</s>) at the end of each sentence, adds 
n-1 sentence-start tokens (<s>) at the beginning of each sentence. It Uses the result to pre-calculate and store the frequency tables
needed by the methods listed below. Store the frequency tables in a variable inside the class. It also includes the method probability,
which returns the n-gram probability of the item we pass it, given the corpus; the method sentence perplexity, which returns the perplexity
of a given sentence; and the method choose successor, which probabilistically chooses a successor using the model. """

#TODO comment pretty and take away examples and jupyter notebook markings
#%%


#Importing necessary modules and methods from said modules 
import re
from corpusreader import CorpusReader
import random

class NgramModel:
    """Accepts a list of tokenized sentences in the form generated by CorpusReader.sents(), and a number which indicates how long the n-grams
    will be."""
    def __init__(self,tokenizedsentences,ngramcount):
        self.tokenizedsentences=tokenizedsentences
        self.ngramcount=ngramcount
        #Calling the later methods from within the initializer, to keep code more organized
        self.formatting= self.formatteddata()     
        self.frequency=self.frequencytables()
        
        
    def formatteddata(self):
        """Removes punctuation from the words within the sentences, makes them into lowercase, adds begginign and end of sentence markers."""
        #Copies the original the tokenized sentences list to avoid editing the original permanently
        copyoftokenizedsentences=self.tokenizedsentences.copy()
        #Initializes empty list where the formatted data will be added to
        formattedsentences=[]
        #Loops over each sentence of the copied list
        for sentence in copyoftokenizedsentences:
            #loops over each word of the sentences
            for  word in sentence:
                #If the looped-over word is composed of one or more non-aplhanumeric character(s), it is removed from the list
                if re.match('\W+',word):
                    sentence.remove(word)    
            #Words are converted to lowercase
            sentence = [word.lower() for word in sentence]
            #An end of sentence marker (</s>) is added at the end of each sentence
            sentence.append('</s>')
            #A beginning of sentence marker (<s>) is added at the beginning of each sentence
            sentence.insert(0,'<s>')
            #The formatted sentences are appendend to the final list
            formattedsentences.append(sentence)
        return formattedsentences     
        
        
    def frequencytables(self):
        """Pre-calculates and stores the frequency tables needed for the later defined methods.
        Returns: dictionary with the n-grams divided as per the number given in self.ngramcount, and respective counts of occurences
        in the corpus."""
        #Initializes list where n-grams will be appended to
        ngramlist=[]
        #Loops over each sentence that has been previously formatted
        for sentence in self.formatting:
            #Creates the first n-gram depending on the ngramcount that was specified
            x=0
            y=self.ngramcount
            #While the y value is shorter than the last indexed item of the sentence, it creates all the possible n-grams 
            #by adding 1 to each coordinate of the index slice, and appends them to the n-gram list
            while y<len(sentence)-1:
                ngramlist.append(tuple(sentence[x:y]))
                x+=1
                y+=1
        #Initiates the frequency dictionary
        frequencytable={}      
        #Loops over each n-gram on the list
        for ngram in ngramlist:
            #If it is already on the dictionary, it counts one more on the value of said n-gram entry
            if ngram in frequencytable:
                frequencytable[ngram]+=1
            #If it is not yet in the dictionary, it is added with a count of one
            else:
                frequencytable[ngram]=1   
        #Returns the frequency dictionary     
        return frequencytable             
    
    
    def probability(self,ngram,smoothing_constant=0.0):
        """Returns the probability of a given n-gram considering the whole corpus. If not specified, assumes a smoothing_constant of 0.0"""
        #Sum of the individual ngrams that share the same prefix
        sumofprefix=0
        #Sum of all of the counts of each individual ngram that share the same prefix
        sumofprefixcounts=0
        #Calls the frequency dictionary that was previously created from the corpus
        currentdictionary=self.frequency
        #If the ngram is present in the dictionary
        if ngram in currentdictionary.keys():
            #Loops over the keys
            for key in currentdictionary.keys():
                #If the prefix of the key matches the one of the ngram that was given, adds the counts of that ngram to the counter of all occurences of that prefix
                if key[0]==ngram[0]:
                    sumofprefixcounts+=currentdictionary[ngram]
                    sumofprefix+=1
            #If the smoothing constant is not specified, it uses the default value and calculates the raw probability
            if smoothing_constant==0.0:
                rawprobability= self.frequency[ngram]/sumofprefixcounts
                return rawprobability
            # If the smoothing constant is specified, it uses that value and calculates the k-smoothed probability, which also uses the individual uses of the prefix
            else:
                ksmoothedprobability= (self.frequency[ngram]+smoothing_constant)/((sumofprefixcounts+smoothing_constant)*sumofprefix)
                return ksmoothedprobability
        #If the ngram is not found in the corpus, it returns probability 0.0
        else:
            return (0.0)
        
# TODO fix and comment perplexity also to understand the issues better. test each section
    def perplexity(self,sentence, smoothing_constant=1.0):
        cleansentence=[]
        #no multiply fpr zero must 1
        
        for word in sentence:
            if re.match('\W+',word):
                sentence.remove(word)    
        #converts all to lowercase
        sentence = [word.lower() for word in sentence]
        #adds 1 sentence-end token (</s>) at the end of each sentence
        sentence.append('</s>')
         #adds n-1 sentence-start tokens (<s>) at the beginning of each sentence,
        sentence.insert(0,'<s>')
        #sum of the counts of each ngram that share the same prefix
        counter=0
        perplexity=1
        for cleanword in sentence:
            counter+=1
            probability=self.probability(word,smoothing_constant)
            perplexity=1/probability
        perplexity**counter
        #print(perplexity)
        #try:
            # perplexity=1/(perplexity*getvalue)
            # return perplexity
        # except:
        #     return float("inf")
    
        
        
        
    # def choose_successor(self, prefix)
          




       
       
       
       
corpus = CorpusReader("C:/Users/ritav/OneDrive - Universiteit Utrecht/A computational linguistics/train")
sentences = corpus.sents()  # a list of lists of tokens
test=NgramModel(sentences,2)
#print(test.perplexity(['The','Ball','and','.']))
print(test.probability('The','Ball'))

# %%
