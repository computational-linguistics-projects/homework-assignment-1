"""This file defines the class NgramModel. It accepts a list of tokenized sentences (in the form generated by CorpusReader.sents()).
It removes punctuation tokens, converts all words to lower case, adds 1 sentence-end token (</s>) at the end of each sentence, adds 
n-1 sentence-start tokens (<s>) at the beginning of each sentence. It Uses the result to pre-calculate and store the frequency tables
needed by the methods listed below. Store the frequency tables in a variable inside the class. It also includes the method probability,
which returns the n-gram probability of the item we pass it, given the corpus; the method sentence perplexity, which returns the perplexity
of a given sentence; and the method choose successor, which probabilistically chooses a successor using the model. """

#TODO comment pretty and take away examples and jupyter notebook markings CLEAN ALL
#%%


#Importing necessary modules and methods from said modules 
import re
from corpusreader import CorpusReader
import random

class NgramModel:
    """Accepts a list of tokenized sentences in the form generated by CorpusReader.sents(), and a number which indicates how long the n-grams
    will be."""
    def __init__(self,tokenizedsentences,ngramcount):
        self.tokenizedsentences=tokenizedsentences
        self.ngramcount=ngramcount
        #Calling the later methods from within the initializer, to keep code more organized
        self.formatting= self.formatteddata()     
        self.frequency=self.frequencytables()
        self.unigram=self.unigram()
        
    def formatteddata(self):
        """Removes punctuation from the words within the sentences, makes them into lowercase, adds begginign and end of sentence markers."""
        #Copies the original the tokenized sentences list to avoid editing the original permanently
        copyoftokenizedsentences=self.tokenizedsentences.copy()
        #Initializes empty list where the formatted data will be added to
        formattedsentences=[]
        #Loops over each sentence of the copied list
        for sentence in copyoftokenizedsentences:
            #loops over each word of the sentences
            for  word in sentence:
                #If the looped-over word is composed of one or more non-aplhanumeric character(s), it is removed from the list
                if re.match('\W+',word):
                    sentence.remove(word)    
            #Words are converted to lowercase
            sentence = [word.lower() for word in sentence]
            #An end of sentence marker (</s>) is added at the end of each sentence
            sentence.append('</s>')
            #A beginning of sentence marker (<s>) is added at the beginning of each sentence
            sentence.insert(0,'<s>')
            #The formatted sentences are appendend to the final list
            formattedsentences.append(sentence)
        return formattedsentences     
        
        
    def frequencytables(self):
        """Pre-calculates and stores the frequency tables needed for the later defined methods. Takes a number, the ngram count.
        Returns: dictionary with the n-grams divided as per the number given in self.ngramcount, and respective counts of occurences
        in the corpus."""
        #Initializes list where n-grams will be appended to
        ngramlist=[]
        #Loops over each sentence that has been previously formatted
        for sentence in self.formatting:
            #Creates the first n-gram depending on the ngramcount that was specified
            x=0
            y=self.ngramcount
            #While the y value is shorter than the last indexed item of the sentence, it creates all the possible n-grams 
            #by adding 1 to each coordinate of the index slice, and appends them to the n-gram list
            while y<len(sentence)-1:
                ngramlist.append(tuple(sentence[x:y]))
                x+=1
                y+=1
        #Initiates the frequency dictionary
        frequencytable={}      
        #Loops over each n-gram on the list
        for ngram in ngramlist:
            #If it is already on the dictionary, it counts one more on the value of said n-gram entry
            if ngram in frequencytable:
                frequencytable[ngram]+=1
            #If it is not yet in the dictionary, it is added with a count of one
            else:
                frequencytable[ngram]=1   
        #Returns the frequency dictionary     
        return frequencytable    
            
            
    def unigram(self):
        unigrams={}
        for sentence in self.formatting:
            for token in sentence:
                if token in unigrams:
                    unigrams[token]+=1
                    
                else:   
                    unigrams[token]=1
        return unigrams       
            
    
    def probability(self,ngram,smoothing_constant=0.0):
        """Returns the probability of a given n-gram considering the whole corpus. If not specified, assumes a smoothing_constant of 0.0"""
        #Sum of the individual ngrams that share the same prefix
        sumofprefix=0
        #Sum of all of the counts of each individual ngram that share the same prefix
        sumofprefixcounts=0
        #Counter for all unique words in the corpus
        #Calls the frequency dictionary that was previously created from the corpus
        currentdictionary=self.frequency
        #Gets the counts
        if ngram in currentdictionary.keys():
            #Loops over the keys
            for key in currentdictionary.keys():
                #The prefix is composed of all tokens except the last. If the prefix of the key matches the one of the ngram that was given, adds the counts of that ngram to the counter of all occurences of that prefix
                if ngram[:-1]==key[:-1]:
                    sumofprefixcounts+=currentdictionary[key]
                    sumofprefix+=1
            #If the smoothing constant is not specified, it uses the default value and calculates the raw probability
            if smoothing_constant==0.0:
                rawprobability= self.frequency[key]/sumofprefixcounts
                return rawprobability
            # If the smoothing constant is specified, it uses that value and calculates the k-smoothed probability, which also uses the individual uses of the prefix
            else:
                ksmoothedprobability= (self.frequency[key]+smoothing_constant)/((sumofprefixcounts+smoothing_constant)*len(self.unigram))
                return ksmoothedprobability
        #If the ngram is not found in the corpus, it returns probability 0.0
        else:   
            return smoothing_constant
        
# TODO fix and comment perplexity also to understand the issues better. test each section
    def perplexity(self,sentence, smoothing_constant=1.0):
        """Given a sentence in the form of a list of tokens, formats it the same way that formatteddata() does,
        removing punctuation-only tokens, changing it to lowercase, and surrounding with <s> and </s>). 
        It calculates and returns the sentence's perplexity using the add-k-smoothed probability model. If unknown
        words are seen, returns infinite."""
        #Intiates a list for the formatted sentence to be appended to
        cleansentence=[]
        #Loops over each word in the given sentence and removes punctuation
        for word in sentence:
            if re.match('\W+',word):
                sentence.remove(word)    
        #Converts all words to lowercase
        sentence = [word.lower() for word in sentence]
        #Adds a sentence-end token (</s>) at the end of the sentence
        sentence.append('</s>')
        #Adds a sentence-start token (<s>) at the beginning of the sentence
        sentence.insert(0,'<s>')
        #Initiating perplexity variable as 1, given that it will be multiplied instead of summed
        perplexity=1
        #Loops over each token in the formatted sentence
        for token in sentence:
            #Calculates the probability of each token, looking through the unigram dictionary, and passing it the default 
            # value of this method as the smoothing constant (1.0; LaPlace smoothing)
            if token in self.unigram.keys():
                tokenprobability=self.probability(token,smoothing_constant)
                perplexity=perplexity*tokenprobability
            else: 
                return float('inf') 
            #Starts to calculate perplexity by multiplying the token's probability one by one
        #After the probabilities have been multiplied, finishes the perplexity calculation by raising it to the power of -1/counter
        perplexity=perplexity**-(1/len(self.unigram.keys()))
        return perplexity

    
        
        
        
    # def choose_successor(self, prefix)
          




       
       
       
       
corpus = CorpusReader("C:/Users/ritav/OneDrive - Universiteit Utrecht/A computational linguistics/train")
sentences = corpus.sents()  # a list of lists of tokens
test=NgramModel(sentences,2)    
print(test.probability(('the','ball')))
print(test.frequency)


print(test.perplexity(['The','Ball','and','.']))

#print(test.unigram())

# %%
